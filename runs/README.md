## Explanation of files in folder.

For reference, in this folder we have included the input files used to generate the l-bit data for the 1D, 2D, and 3D disordered Heisenberg model and 2D hard-core Bose-Hubbard models from our paper. These input files are provided to the `run_disordered_heisenberg_scan.py` (or `run_disordered_bosehubbard_scan.py`) scripts to generate the data. To do this efficiently, we use MPI (which requires the mpi4py package and MPI installed on your machine).

## Workflow for generating MBL l-bit data.

Our procedure for generating our data was:

1. Create an `input_{run_name}.json` file, where {run_name} is replaced by the identifying name of the run.
2. Execute a parallel run with a command like
```
aprun -n 1600 -N 20 -d 1 python3 -u run_disordered_heisenberg_scan.py -I input_1D_heisenberg1.json -S 1 -P 0
```
using either "aprun", "mpirun", or "mpiexec" depending on how MPI is setup on the machine. This can be executed in the terminal or using a script. In this particular example, I am running 1600 samples of random disordered 1D Heisenberg models with 20 MPI processes per node and 1 core per MPI process. This will create 1600 folders, one for each sample, in which text and python pickle files will be stored as output. To run the Bose-Hubbard instead of the Heisenberg model use `run_disordered_bosehubbard_scan.py` instead.
3. Create a `input_collect_{run_name}.json` file. The `setup_collect_input_file.py` script can help create one.
4. Run a command like
```
aprun -n 320 -N 32 -d 1 python3 -u collect_run_data.py -I input_collect_run1D_heisenberg1.json
```
again using either "aprun", "mpirun", or "mpiexec" depending on how MPI is setup on the machine. In this example, I am using 320 MPI processes, with 32 processes per node, and 1 core per MPI process. This will collect all of the output files stored in the 1600 folders and from it generate many `output_{run_name}_{n}.p` files, where {n}=0,...,319 in this case. (Note: not all of the data stored in the folders is stored in these output files. In particular, the files in the folders have information about each iteration of the gradient descent calculations, while the new output files only have the final converged results of the gradient descent.)
5. Run a command like
```
python3 combine_data_frames.py -I input_collect_run1d_heisenberg1.json -N 320
```
to combine the `output_{run_name}_{n}.p` files into a single `output_{run_name}.p` file.
6. Modify the `compress_data_frames.py` to change what `output_{run_name}.p` files you want to run it on. Run
```
python3 compress_data_frames.py
```
This stores a compressed version of `output_{run_name}.p` into the folder `compressed_data`.

## Guide for data stored in output files.

The raw output generated by `run_disordered_heisenberg_scan.py` (and `run_disordered_bosehubbard_scan.py`) are pickle files, which contain dictionaries storing data. The `output_{run_name}.p` file is a pickled Pandas DataFrame, whose entries are also indexed by the same string keys as in the raw output. Each row of the DataFrame corresponds to a different \\tau^z_i operator. For reference, descriptions of the most important keys labeling the properties of \\tau^z_i are listed here:

| key | Description |
| --- | ----------- |
| 'L' | The system side-length. (Ex: number of spins = L^2 in 2D.) |
| 'W' | The disorder strength. (Stands for the FWHM \Delta in the 2D Bose-Hubbard model.) |
| 'random_potentials'| The disorder pattern of the magnetic fields in the run. |
| 'ham_type' | Parameter specifying the Hamiltonian's dimensionality.  |
| 'basis_size' | The size of the basis |B| of Pauli strings used to represent \\tau^z_i. |
| 'ind_expansion' | The iteration of the basis expansion algorithm. Starts at 0. |
| 'num_expansions' | The number of iterations of the basis expansion algorithm. |
| 'dbasis' | The (maximum) number of Pauli strings to add to the basis during each expansion iteration. |
| 'truncation_size' | The number of Pauli strings to keep in [H, \\tau^z_i] and (\tau^z_i)^2 - I during basis expansion. |
| 'obj' | The objective function value of \alpha * |[H, \\tau^z_i]|^2 + \beta * |(\\tau^z_i)^2 - I|^2. |
| 'com_norm' | The commutator norm |[H, \\tau^z_i]|^2. |
| 'binarity' | The binarity |(\\tau^z_i)^2 - I|^2. |
| 'coeff_com_norm'| The coefficient \alpha. |
| 'coeff_binarity' | The coefficient \beta. |
| 'tau_norm' | The norm |\\tau^z_i| of the \\tau^z_i operator. |
| 'weights'  | The weights w_x of the \\tau^z_i operator. This is a sparse scipy.csr_matrix indexed by site. |
| 'coeff_sqr_center' | The overlap |\langle \\tau^z_i, \\sigma^z_i \rangle|^2 with the site with the largest weight. |
| 'corr_length' | The correlation length \xi obtained from non-linear least-squares fitting of the weights. |
| 'average_range' | The range r in the paper's supplement. |
| 'op_ipr' | The operator inverse participation ratio in the paper's supplement. |
| 'center_site' | The site (spin) at which the weight is maximal. |
| 'initial_fidelity' | The (squared) overlap of \\tau^z_i with the initial operator before basis expansion. |
| 'final_fidelity' | The (squared) overlap of \\tau^z_i with the final operator after all basis expansions. |
| 'folder' | The folder in which the particular run's data is stored. |
| 'num_samples' | The number of disordered realizations generated in the combined run. |
| 'seed' | Random number seed used to generate magnetic field disorder patterns. |
| 'xtol' | Parameter specifying convergence criteria for Newton's method gradient descent. |
| 'verbose' | Parameter specifying whether to print output of run to a text file. |
| 'out_name' | The name of the output text file to stored the print output of the run. |
| 'proc' | The processor number of the processor executing this run. |
